include classpath("service-commons.base.conf")

slick.dbs.default {
  profile = "slick.jdbc.PostgresProfile$"
  driver = "org.postgresql.Driver"
  db {
    url = "jdbc:postgresql://db:5432/storage"
    url = ${?STORAGE_BACKEND_DB_URL}
    user = "storage"
    user = ${?STORAGE_BACKEND_DB_USER}
    password = "storage"
    password = ${?STORAGE_BACKEND_DB_PASSWORD}
  }
}

play.evolutions.autoApplyDowns=false

play.modules.enabled += "modules.BackendModule"
play.modules.enabled += "modules.EventPublisherModule"
//play.modules.enabled += "modules.EventContextModule"
play.http.filters = "Filters"

pidfile.path = "/dev/null"
play.http.context="/api/storage"

storage.backend.local.enabled = false
storage.backend.local.enabled = ${?STORAGE_BACKEND_LOCAL_ENABLED}
storage.backend.local {
  root = "/data/obj"
  root = ${?LOCAL_STORAGE_ROOT}
}

storage.backend.swift.enabled = false
storage.backend.swift.enabled = ${?STORAGE_BACKEND_SWIFT_ENABLED}
storage.backend.swift {
  username = ${?SWIFT_USERNAME}
  password = ${?SWIFT_PASSWORD}
  auth_url = ${?SWIFT_AUTH_URL}
  project  = ${?SWIFT_PROJECT}
}

storage.backend.s3.enabled = false
storage.backend.s3.enabled = ${?STORAGE_BACKEND_S3_ENABLED}
storage.backend.s3 {
  url = ${?S3_URL}
  access_key = ${?S3_ACCESS_KEY}
  secret_key = ${?S3_SECRET_KEY}
}

storage.backend.azure.enabled = false
storage.backend.azure.enabled = ${?STORAGE_BACKEND_AZURE_ENABLED}
storage.backend.azure {
  connection_string = ${?AZURE_CONNECTION_STRING}
}

storage.backend.localgit.enabled = false
storage.backend.localgit.enabled = ${?STORAGE_BACKEND_LOCALGIT_ENABLED}
storage.backend.localgit.root = "/data/repo"
storage.backend.localgit.root = ${?LOCAL_STORAGE_GIT_ROOT}

renku_host = "http://localhost"
renku_host = ${?RENKU_ENDPOINT}

lfs_default_backend = "local"
lfs_default_backend =  ${?RENKU_STORAGE_DEFAULT_LFS_BACKEND}

storage.backend.gitlab.enabled = false
storage.backend.gitlab.enabled = ${?STORAGE_BACKEND_GITLAB_ENABLED}
storage.backend.gitlab {
  url = ${?GITLAB_URL}
  username = ${?GITLAB_USER}
  pass = ${?GITLAB_PASS}
}


kafka {
  bootstrap.servers = "172.17.0.1:9092,"
  client.id = "storage_events"
  group.id = "storage_events"
  enable.auto.commit = "false"
  key.serializer = "org.apache.kafka.common.serialization.LongSerializer"
  value.serializer = "org.apache.kafka.common.serialization.ByteArraySerializer"
  key.deserializer = "org.apache.kafka.common.serialization.LongDeserializer"
  value.deserializer = "org.apache.kafka.common.serialization.ByteArrayDeserializer"

  topics {
    storage_events {
      partitions = 1
      replication = 1
    }
  }
}

# Push to storage_events topics
events.push_to = "storage_events"
events.fetch_size = 1

event-publisher {
  # Dispatcher is the name of the event-based dispatcher
  type = Dispatcher
  # What kind of ExecutionService to use
  executor = "fork-join-executor"
  # Configuration for the fork join pool
  fork-join-executor {
    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 2
    # Parallelism (threads) ... ceil(available processors * factor)
    parallelism-factor = 2.0
    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 10
  }
  # Throughput defines the maximum number of messages to be
  # processed per actor before the thread jumps to the next actor.
  # Set to 1 for as fair as possible.
  throughput = 100
}